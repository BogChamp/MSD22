{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip3 install pytorch-lightning"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-DA4SXATGFVJ",
        "outputId": "714a1f60-f8bd-4e31-873c-a4ffd9d8ad32"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pytorch-lightning in /usr/local/lib/python3.8/dist-packages (1.8.5.post0)\n",
            "Requirement already satisfied: tensorboardX>=2.2 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning) (2.5.1)\n",
            "Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning) (6.0)\n",
            "Requirement already satisfied: lightning-utilities!=0.4.0,>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning) (0.4.2)\n",
            "Requirement already satisfied: torchmetrics>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning) (0.11.0)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning) (4.4.0)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning) (21.3)\n",
            "Requirement already satisfied: torch>=1.9.0 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning) (1.13.0+cu116)\n",
            "Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning) (4.64.1)\n",
            "Requirement already satisfied: fsspec[http]>2021.06.0 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning) (2022.11.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from fsspec[http]>2021.06.0->pytorch-lightning) (2.23.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.8/dist-packages (from fsspec[http]>2021.06.0->pytorch-lightning) (3.8.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (22.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (1.3.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (1.8.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (6.0.3)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (2.1.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (4.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=17.0->pytorch-lightning) (3.0.9)\n",
            "Requirement already satisfied: protobuf<=3.20.1,>=3.8.0 in /usr/local/lib/python3.8/dist-packages (from tensorboardX>=2.2->pytorch-lightning) (3.19.6)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.8/dist-packages (from yarl<2.0,>=1.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning) (2022.12.7)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning) (3.0.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "4DXh_ZVjJ4H2"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "\n",
        "from math import floor\n",
        "import pandas as pd\n",
        "import scipy.io as sio\n",
        "import csv\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.metrics import confusion_matrix,f1_score\n",
        "import pytorch_lightning as pl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "7Tlj4WxAKuBf"
      },
      "outputs": [],
      "source": [
        "def load_hasc_ds(path, window, mode='train'):\n",
        "\n",
        "    X, lbl = extract_windows_hasc(path, window, 5)\n",
        "\n",
        "    if mode == \"all\":\n",
        "        return X, lbl\n",
        "\n",
        "    train_size = floor(0.8 * X.shape[0])\n",
        "    print(train_size)\n",
        "    if mode ==\"train\":\n",
        "        trainx = X[0:train_size]\n",
        "        trainlbl =lbl[0:train_size]\n",
        "        idx = np.arange(trainx.shape[0])\n",
        "        np.random.shuffle(idx)\n",
        "        trainx = trainx[idx,]\n",
        "        trainlbl = trainlbl[idx]\n",
        "        print('train samples : ', train_size)\n",
        "        return trainx, trainlbl\n",
        "\n",
        "    else:\n",
        "        testx = X[train_size:]\n",
        "        testlbl = lbl[train_size:]\n",
        "        print('test shape {} and number of change points {} '.format(testx.shape, len(np.where(testlbl>0)[0])))\n",
        "\n",
        "        return testx, testlbl\n",
        "\n",
        "def extract_windows_hasc(path, window_size, step):\n",
        "    dataset = sio.loadmat(path+\"hasc.mat\")\n",
        "    window_size\n",
        "    windows = []\n",
        "    lbl = []\n",
        "    first = True\n",
        "    num_cp = 0\n",
        "    x = np.array(dataset['Y'])\n",
        "    cp = np.array(dataset['L'])\n",
        "\n",
        "    ts = np.sqrt(np.power(x[:, 0], 2) + np.power(x[:, 1], 2) + np.power(x[:, 2], 2))\n",
        "    for i in range(0, ts.shape[0] - window_size, window_size // 5):\n",
        "        windows.append(np.array(ts[i:i + window_size]))\n",
        "        is_cp = np.where(cp[i:i + window_size] == 1)[0]\n",
        "        if is_cp.size == 0:\n",
        "            is_cp = [0]\n",
        "        else:\n",
        "            num_cp += 1\n",
        "        lbl.append(is_cp[0])\n",
        "\n",
        "\n",
        "    print(\"number of samples : {} /  number of samples with change point : {}\".format(len(windows), num_cp))\n",
        "    windows = np.array(windows)\n",
        "\n",
        "    return windows, np.array(lbl)\n",
        "\n",
        "def load_usc_ds(path, window, mode='train'):\n",
        "\n",
        "    X, lbl = extract_windows_usc(path, window, mode)\n",
        "\n",
        "    if mode == \"all\":\n",
        "        return X, lbl\n",
        "    train_size = int(floor(0.8 * X.shape[0]))\n",
        "    if mode == \"train\":\n",
        "        trainx = X[0:train_size]\n",
        "        trainlbl = lbl[0:train_size]\n",
        "        idx = np.arange(trainx.shape[0])\n",
        "        np.random.shuffle(idx)\n",
        "        trainx = trainx[idx,]\n",
        "        trainlbl = trainlbl[idx]\n",
        "        print('train samples : ', train_size)\n",
        "        return trainx, trainlbl\n",
        "\n",
        "    else:\n",
        "        testx = X[train_size:]\n",
        "        testlbl = lbl[train_size:]\n",
        "        print('test shape {} and number of change points {} '.format(testx.shape, len(np.where(testlbl > 0)[0])))\n",
        "\n",
        "        return testx, testlbl\n",
        "\n",
        "\n",
        "def extract_windows_usc(path, window_size, mode=\"train\"):\n",
        "    windows = []\n",
        "    lbl = []\n",
        "    dataset = sio.loadmat(path+\"usc.mat\")\n",
        "\n",
        "    ts = np.array(dataset['Y'])\n",
        "    ts = ts[:,0]\n",
        "    cp = np.array(dataset['L'])\n",
        "    cp = cp[:,0]\n",
        "\n",
        "    num_cp = 0\n",
        "    for i in range(0, ts.shape[0] - window_size, window_size // 5):\n",
        "        windows.append(np.array(ts[i:i + window_size]))\n",
        "        is_cp = np.where(cp[i:i + window_size] == 1)[0]\n",
        "        if is_cp.size == 0:\n",
        "            is_cp = [0]\n",
        "        else:\n",
        "            num_cp += 1\n",
        "        lbl.append(is_cp[0])\n",
        "\n",
        "    print(\"number of samples : {} /  number of samples with change point : {}\".format(len(windows), num_cp))\n",
        "    windows = np.array(windows)\n",
        "\n",
        "    return windows, np.array(lbl)\n",
        "\n",
        "def load_dataset(path, ds_name, win, bs, mode=\"train\"):\n",
        "    if ds_name == 'HASC':\n",
        "        trainx, trainlbl = load_hasc_ds(path, window = 2 * win, mode=mode)\n",
        "    elif ds_name == \"USC\":\n",
        "        trainx, trainlbl = load_usc_ds(path, window=2 * win, mode=mode)\n",
        "    else:\n",
        "        raise ValueError(\"Undefined Dataset.\")\n",
        "\n",
        "    trainlbl = trainlbl.reshape((trainlbl.shape[0], 1))\n",
        "    print(trainx.shape, trainlbl.shape)\n",
        "    dataset = np.concatenate((trainlbl, trainx), 1)\n",
        "\n",
        "    print(\"dataset shape : \", dataset.shape)\n",
        "    if mode == \"test\":\n",
        "        return dataset\n",
        "\n",
        "    train_ds = TensorDataset(torch.from_numpy(dataset))\n",
        "    return train_ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "UBIkGMvClX1R"
      },
      "outputs": [],
      "source": [
        "def estimate_CPs(sim, gt, name, train_name, metric='cosine', threshold=0.5):\n",
        "    est_cp = np.zeros(sim.shape[0])\n",
        "    est_cp[np.where(sim < threshold)[0]] = 1\n",
        "    tn, fp, fn, tp = confusion_matrix(gt, est_cp).ravel()\n",
        "    f1 = f1_score(gt, est_cp)\n",
        "\n",
        "    gt_id = np.where(gt == 1)[0]\n",
        "    print(\"tn {}, fp {}, fn {}, tp {} ----- f1-score {}\".format(tn, fp, fn, tp, f1))\n",
        "\n",
        "    i = 1\n",
        "    pos, seq_tp, seq_fn, seq_fp = 0, 0, 0, 0\n",
        "\n",
        "    while i < gt.shape[0]:\n",
        "        if gt[i] == 1:\n",
        "            pos += 1\n",
        "            j = i\n",
        "            while gt[i] == 1:\n",
        "                i += 1\n",
        "\n",
        "            if np.sum(est_cp[j:i]) > 0:\n",
        "                seq_tp += 1\n",
        "                est_cp[j:i] = 0\n",
        "            else:\n",
        "                seq_fn += 1\n",
        "\n",
        "        i += 1\n",
        "\n",
        "    seq_fp = np.where(np.diff(est_cp) == 1)[0].shape[0]\n",
        "    seq_f1 = (2 * seq_tp) / (2 * seq_tp + seq_fn + seq_fp)\n",
        "\n",
        "    print(\"SEQ : Pos {}, fp {}, fn {}, tp {} ----- f1-score {}\".format(pos, seq_fp, seq_fn, seq_tp, seq_f1))\n",
        "    result = \"tn, {}, fp, {}, fn, {}, tp, {}, f1-score, {}, Pos, {}, seqfp, {}, seqfn, {}, seqtp, {}, seqf1, {}\\n\".format(tn, fp, fn, tp, f1, pos, seq_fp, seq_fn, seq_tp, seq_f1)\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "5G5uI840Ekcj"
      },
      "outputs": [],
      "source": [
        "class ResidualBlock(nn.Module):\n",
        "    \n",
        "    def __init__(self,\n",
        "                 in_channels,\n",
        "                 dilation_rate: int,\n",
        "                 nb_filters: int,\n",
        "                 kernel_size: int,\n",
        "                 dropout_rate: float = 0, \n",
        "                 use_batch_norm: bool = False,\n",
        "                 use_layer_norm: bool = False,\n",
        "                 use_weight_norm: bool = False, \n",
        "                 training: bool = True):\n",
        "        \"\"\"Defines the residual block for the WaveNet TCN\n",
        "        Args:\n",
        "            dilation_rate: The dilation power of 2 we are using for this residual block\n",
        "            nb_filters: The number of convolutional filters to use in this block\n",
        "            kernel_size: The size of the convolutional kernel\n",
        "            activation: The final activation used in o = Activation(x + F(x))\n",
        "            dropout_rate: Float between 0 and 1. Fraction of the input units to drop.\n",
        "        \"\"\"\n",
        "\n",
        "        self.dilation_rate = dilation_rate\n",
        "        self.nb_filters = nb_filters\n",
        "        self.kernel_size = kernel_size\n",
        "        self.dropout_rate = dropout_rate\n",
        "        # for causal padding\n",
        "        self.padding = (self.kernel_size - 1) * self.dilation_rate\n",
        "        \n",
        "        self.use_batch_norm = use_batch_norm\n",
        "        self.use_layer_norm = use_layer_norm\n",
        "        self.use_weight_norm = use_weight_norm\n",
        "        \n",
        "        self.training = training\n",
        "\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        \n",
        "        self.conv_1 = nn.Conv1d(in_channels, self.nb_filters, self.kernel_size, \n",
        "                                padding=0, dilation=self.dilation_rate)        \n",
        "        if self.use_weight_norm:\n",
        "            weight_norm(self.conv_1) \n",
        "        self.bn_1 = nn.BatchNorm1d(self.nb_filters)\n",
        "        self.ln_1 = nn.LayerNorm(self.nb_filters)              \n",
        "        self.relu_1 = nn.ReLU()\n",
        "\n",
        "        self.conv_2 = nn.Conv1d(self.nb_filters, self.nb_filters, self.kernel_size, \n",
        "                                padding=0, dilation=self.dilation_rate)        \n",
        "        if self.use_weight_norm:\n",
        "            weight_norm(self.conv_1)    \n",
        "        self.bn_2 = nn.BatchNorm1d(self.nb_filters)\n",
        "        self.ln_2 = nn.LayerNorm(self.nb_filters)              \n",
        "        self.relu_2 = nn.ReLU()        \n",
        "        \n",
        "        self.conv_block = nn.Sequential()\n",
        "        self.downsample = nn.Conv1d(in_channels, self.nb_filters, kernel_size=1) if in_channels != self.nb_filters else nn.Identity()\n",
        "        \n",
        "        self.relu = nn.ReLU()  \n",
        "                \n",
        "        self.init_weights()\n",
        "        \n",
        "        \n",
        "    def init_weights(self):\n",
        "        # in the realization, they use random normal initialization\n",
        "        torch.nn.init.normal_(self.conv_1.weight, mean=0, std=0.05)\n",
        "        torch.nn.init.zeros_(self.conv_1.bias)            \n",
        "        \n",
        "        torch.nn.init.normal_(self.conv_2.weight, mean=0, std=0.05)\n",
        "        torch.nn.init.zeros_(self.conv_2.bias)            \n",
        "        \n",
        "        if isinstance(self.downsample, nn.Conv1d):         \n",
        "            torch.nn.init.normal_(self.downsample.weight, mean=0, std=0.05)\n",
        "            torch.nn.init.zeros_(self.downsample.bias)                    \n",
        "            \n",
        "    def forward(self, inp):\n",
        "        # inp batch, channels, time\n",
        "        ######################\n",
        "        # do causal padding        \n",
        "        out = F.pad(inp, (self.padding, 0))\n",
        "        out = self.conv_1(out)\n",
        "        \n",
        "        if self.use_batch_norm:\n",
        "            out = self.bn_1(out)\n",
        "        elif self.use_layer_norm:\n",
        "            out = self.ln_1(out)        \n",
        "        out = self.relu_1(out)\n",
        "        \n",
        "        # spatial dropout\n",
        "        out = out.permute(0, 2, 1)   # convert to [batch, time, channels]\n",
        "        out = F.dropout2d(out, self.dropout_rate, training=self.training)        \n",
        "        out = out.permute(0, 2, 1)   # back to [batch, channels, time]    \n",
        "        \n",
        "        #######################\n",
        "        # do causal padding\n",
        "        out = F.pad(out, (self.padding, 0))\n",
        "        out = self.conv_2(out)\n",
        "        if self.use_batch_norm:\n",
        "            out = self.bn_2(out)\n",
        "        elif self.use_layer_norm:\n",
        "            out = self.ln_2(out)\n",
        "        out = self.relu_2(out)            \n",
        "        out = self.relu_2(out)    \n",
        "        # spatial dropout\n",
        "        # out batch, channels, time \n",
        "        \n",
        "        out = out.permute(0, 2, 1)   # convert to [batch, time, channels]\n",
        "        out = F.dropout2d(out, self.dropout_rate, training=self.training)\n",
        "        out = out.permute(0, 2, 1)   # back to [batch, channels, time]            \n",
        "        \n",
        "        #######################        \n",
        "        skip_out = self.downsample(inp)\n",
        "        #######################\n",
        "        res = self.relu(out + skip_out)\n",
        "        return res, skip_out\n",
        "    \n",
        "# only causal padding\n",
        "# only return sequence = True\n",
        "    \n",
        "class TCN(nn.Module):        \n",
        "    def __init__(self,\n",
        "                 in_channels=1,\n",
        "                 nb_filters=64,\n",
        "                 kernel_size=3,\n",
        "                 nb_stacks=1,\n",
        "                 dilations=(1, 2, 4, 8, 16, 32),\n",
        "                 use_skip_connections=True,\n",
        "                 dropout_rate=0.0, \n",
        "                 use_batch_norm: bool = False,\n",
        "                 use_layer_norm: bool = False, \n",
        "                 use_weight_norm: bool = False):\n",
        "\n",
        "        super(TCN, self).__init__()\n",
        "        \n",
        "        self.dropout_rate = dropout_rate\n",
        "        self.use_skip_connections = use_skip_connections\n",
        "        self.dilations = dilations\n",
        "        self.nb_stacks = nb_stacks\n",
        "        self.kernel_size = kernel_size\n",
        "        self.nb_filters = nb_filters\n",
        "        \n",
        "        self.use_batch_norm = use_batch_norm\n",
        "        self.use_layer_norm = use_layer_norm\n",
        "        self.use_weight_norm = use_weight_norm\n",
        "        self.in_channels = in_channels\n",
        "        \n",
        "        if self.use_batch_norm + self.use_layer_norm + self.use_weight_norm > 1:\n",
        "            raise ValueError('Only one normalization can be specified at once.')        \n",
        "        \n",
        "        self.residual_blocks = []        \n",
        "        res_block_filters = 0\n",
        "        for s in range(self.nb_stacks):\n",
        "            for i, d in enumerate(self.dilations):\n",
        "                in_channels = self.in_channels if i + s == 0 else res_block_filters                \n",
        "                res_block_filters = self.nb_filters[i] if isinstance(self.nb_filters, list) else self.nb_filters\n",
        "                self.residual_blocks.append(ResidualBlock(in_channels=in_channels, \n",
        "                                                          dilation_rate=d,\n",
        "                                                          nb_filters=res_block_filters,\n",
        "                                                          kernel_size=self.kernel_size,\n",
        "                                                          dropout_rate=self.dropout_rate, \n",
        "                                                          use_batch_norm=self.use_batch_norm,\n",
        "                                                          use_layer_norm=self.use_layer_norm,\n",
        "                                                          use_weight_norm=self.use_weight_norm))\n",
        "\n",
        "        \n",
        "        self.residual_blocks = nn.ModuleList(self.residual_blocks)\n",
        "                                            \n",
        "    def forward(self, inp):\n",
        "        out = inp\n",
        "        for layer in self.residual_blocks:\n",
        "            out, skip_out = layer(out)\n",
        "        if self.use_skip_connections:\n",
        "            out = out + skip_out\n",
        "        return out\n",
        "\n",
        "########################### model #########################################\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, c_in=1, nb_filters=64, kernel_size=4, \n",
        "                 dilations=[1,2,4,8], nb_stacks=2, n_steps=50, code_size=10, seq_len=100):       \n",
        "        super(Encoder, self).__init__()        \n",
        "        \n",
        "        self.tcn_layer = TCN(in_channels=c_in, nb_filters=nb_filters, \n",
        "                             nb_stacks=nb_stacks, dilations=dilations, use_skip_connections=True, dropout_rate=0)\n",
        "        \n",
        "        self.fc1 = nn.Linear(nb_filters * seq_len, 2 * n_steps)  \n",
        "        self.fc2 = nn.Linear(2 * n_steps, n_steps)    \n",
        "        self.output_layer = nn.Linear(n_steps, code_size)           \n",
        "        self.relu = nn.ReLU()\n",
        "        \n",
        "    def forward(self, x):\n",
        "        out = x\n",
        "        if len(out.shape) == 2:\n",
        "            out = out.unsqueeze(1)\n",
        "        out = self.tcn_layer(out)     \n",
        "        out = out.flatten(1, 2)         \n",
        "        out = self.relu(self.fc1(out)) \n",
        "        out = self.relu(self.fc2(out)) \n",
        "        out = self.output_layer(out)\n",
        "        return out\n",
        "    \n",
        "########################### loss #########################################\n",
        "def _cosine_simililarity_dim2(x, y):\n",
        "    # x shape: (N, 1, C)\n",
        "    # y shape: (1, 2N, C)\n",
        "    # v shape: (N, 2N)\n",
        "    cos = nn.CosineSimilarity(dim=2, eps=1e-6)\n",
        "    v = cos(x.unsqueeze(1), y.unsqueeze(0))\n",
        "    return v    \n",
        "\n",
        "def nce_loss_fn(history, future, similarity, temperature=0.5):\n",
        "    try:\n",
        "        device = history.device\n",
        "    except:\n",
        "        device = 'cpu'\n",
        "        \n",
        "    criterion = torch.nn.BCEWithLogitsLoss()\n",
        "    N = history.shape[0]\n",
        "    sim = similarity(history, future)\n",
        "    pos_sim = torch.exp(torch.diag(sim) / temperature)\n",
        "\n",
        "    tri_mask = torch.ones((N, N), dtype=bool)\n",
        "    tri_mask[np.diag_indices(N)] = False\n",
        "    \n",
        "    neg = sim[tri_mask].reshape(N, N - 1)    \n",
        "    all_sim = torch.exp(sim / temperature)\n",
        "    \n",
        "    logits = torch.divide(torch.sum(pos_sim), torch.sum(all_sim, axis=1))\n",
        "        \n",
        "    lbl = torch.ones(history.shape[0]).to(device)\n",
        "\n",
        "    loss = criterion(logits, lbl)    \n",
        "    mean_sim = torch.mean(torch.diag(sim))\n",
        "    mean_neg = torch.mean(neg)\n",
        "    return loss, mean_sim, mean_neg\n",
        "\n",
        "\n",
        "######################### preprocessing ###########################\n",
        "\n",
        "def _history_future_separation(mbatch, win):    \n",
        "    x = mbatch[:,1:win+1]\n",
        "    y = mbatch[:,-win:]\n",
        "    return x, y\n",
        "\n",
        "\n",
        "################## PL wrapper ###############################################\n",
        "class TSCP_model(pl.LightningModule):\n",
        "    def __init__(\n",
        "        self,\n",
        "        model: nn.Module,     \n",
        "        train_dataset: Dataset, \n",
        "        test_dataset: Dataset, \n",
        "        batch_size: int = 64,        \n",
        "        num_workers: int = 2,        \n",
        "        temperature: float = 0.1, \n",
        "        lr: float = 1e-4,\n",
        "        decay_steps: int = 1000, \n",
        "        window_1: int = 100,\n",
        "        window_2: int = 100\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "                    \n",
        "        self.model = model\n",
        "        self.train_dataset = train_dataset\n",
        "        self.test_dataset = test_dataset\n",
        "        \n",
        "        self.batch_size = batch_size\n",
        "        self.num_workers = num_workers        \n",
        "        \n",
        "        self.temperature = temperature\n",
        "        \n",
        "        self.lr = lr\n",
        "        self.decay_steps = decay_steps   \n",
        "        \n",
        "        self.window = window_1\n",
        "        self.window_1 = window_1\n",
        "        self.window_2 = window_2\n",
        "\n",
        "    def forward(self, inputs: torch.Tensor) -> torch.Tensor:\n",
        "        return self.model(inputs)\n",
        "    \n",
        "    def training_step(self, batch, batch_idx):\n",
        "\n",
        "        history, future = _history_future_separation(batch[0], self.window)        \n",
        "        history_emb = self.forward(history.float())\n",
        "        future_emb = self.forward(future.float()) \n",
        "\n",
        "        history_emb = nn.functional.normalize(history_emb, p=2, dim=1)\n",
        "        future_emb = nn.functional.normalize(future_emb, p=2, dim=1)\n",
        "\n",
        "        train_loss, pos_sim, neg_sim = nce_loss_fn(history_emb, future_emb, similarity=_cosine_simililarity_dim2, \n",
        "                                                   temperature=self.temperature)\n",
        "\n",
        "        return train_loss, pos_sim, neg_sim\n",
        "        \n",
        "    def validation_step(self, batch, batch_idx):\n",
        "\n",
        "        history, future = _history_future_separation(batch, self.window)        \n",
        "                \n",
        "        history_emb = self.forward(history.float())\n",
        "        future_emb = self.forward(future.float()) \n",
        "                \n",
        "        history_emb = nn.functional.normalize(history_emb, p=2, dim=1)\n",
        "        future_emb = nn.functional.normalize(future_emb, p=2, dim=1)\n",
        "        \n",
        "\n",
        "        val_loss, pos_sim, neg_sim = nce_loss_fn(history_emb, future_emb, similarity=_cosine_simililarity_dim2, \n",
        "                                                 temperature=self.temperature)      \n",
        "\n",
        "        return val_loss, pos_sim, neg_sim\n",
        "\n",
        "\n",
        "    def configure_optimizers(self) -> torch.optim.Optimizer:\n",
        "        \"\"\"Initialize optimizer.\n",
        "\n",
        "        :return: optimizer for training CPD model\n",
        "        \"\"\"\n",
        "        opt = torch.optim.Adam(self.model.parameters(), lr=self.lr)\n",
        "        return opt\n",
        "\n",
        "    def train_dataloader(self) -> DataLoader:\n",
        "        return DataLoader(\n",
        "            self.train_dataset, batch_size=self.batch_size, shuffle=True, num_workers=self.num_workers\n",
        "        )\n",
        "\n",
        "    def val_dataloader(self) -> DataLoader:\n",
        "        return DataLoader(\n",
        "            self.test_dataset, batch_size=self.batch_size, shuffle=False, num_workers=self.num_workers\n",
        "        )\n",
        "    \n",
        "def _cosine_simililarity_dim1(x, y):\n",
        "    cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
        "    v = cos(x, y)\n",
        "    return v    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mRB-0W_1JJpE",
        "outputId": "5733e8e0-1ba3-4be3-fe0e-3255a82e019e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------>>> CP2_model_USC_T0.5_WIN100_BS32_CS10_lr0.0001_LOSSnce_SIMcosine_TAU0.1_BETA1\n",
            "number of samples : 2336 /  number of samples with change point : 175\n",
            "train samples :  1868\n",
            "(1868, 200) (1868, 1)\n",
            "dataset shape :  (1868, 201)\n",
            "number of samples : 2336 /  number of samples with change point : 175\n",
            "test shape (468, 200) and number of change points 35 \n",
            "(468, 200) (468, 1)\n",
            "dataset shape :  (468, 201)\n"
          ]
        }
      ],
      "source": [
        "DS_NAME = 'USC'\n",
        "DATA_PATH = './data/'\n",
        "OUTPUT_PATH = os.path.join('./output/', DS_NAME)\n",
        "MODEL_PATH = os.path.join('./output/', \"model\")\n",
        "LOSS = 'nce'\n",
        "SIM = 'cosine'\n",
        "GPU = 0\n",
        "\n",
        "WIN = 100\n",
        "CODE_SIZE = 10\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 30\n",
        "LR = 1e-4\n",
        "TEMP = 0.5\n",
        "TAU = 0.1\n",
        "BETA = 1\n",
        "EVALFREQ = 25\n",
        "decay_steps = 1000\n",
        "\n",
        "\n",
        "train_name = \"CP2_model_\" + DS_NAME + \"_T\" + str(TEMP) + \"_WIN\" + str(WIN) + \\\n",
        "             \"_BS\" + str(BATCH_SIZE) + \"_CS\" + str(CODE_SIZE) + \"_lr\" + str(LR) + \\\n",
        "             \"_LOSS\" + LOSS +  \"_SIM\" + SIM + \"_TAU\" + str(TAU) + \"_BETA\" + str(BETA)\n",
        "print(\"------------------------------------>>> \" + train_name)\n",
        "\n",
        "train_ds = load_dataset(DATA_PATH, DS_NAME, WIN, BATCH_SIZE, mode = \"train\")\n",
        "test_ds = load_dataset(DATA_PATH, DS_NAME, WIN, BATCH_SIZE, mode = \"test\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "bSTDxTIoJJlZ"
      },
      "outputs": [],
      "source": [
        "prep_model = Encoder(code_size = CODE_SIZE, seq_len = WIN)\n",
        "\n",
        "\n",
        "tscp_model = TSCP_model(prep_model, train_ds, test_ds, batch_size=BATCH_SIZE, temperature=TEMP, lr=LR, decay_steps=decay_steps, window_1=WIN)\n",
        "optimizer = tscp_model.configure_optimizers()\n",
        "lr = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer=optimizer, T_max=decay_steps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sygpKdzGJJgX",
        "outputId": "dc92a707-57a3-4b68-c252-91c840013193"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/30 [00:00<?, ?it/s]/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:1338: UserWarning: dropout2d: Received a 3D input to dropout2d and assuming that channel-wise 1D dropout behavior is desired - input is interpreted as shape (N, C, L), where C is the channel dim. This behavior will change in a future release to interpret the input as one without a batch dimension, i.e. shape (C, H, W). To maintain the 1D channel-wise dropout behavior, please switch to using dropout1d instead.\n",
            "  warnings.warn(\"dropout2d: Received a 3D input to dropout2d and assuming that channel-wise \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_losses_iters 0.3132399618625641\n",
            "val_loss 0.3040047499040763\n",
            "train_losses_iters 0.12153904139995575\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  3%|▎         | 1/30 [00:21<10:37, 21.97s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch_train_loss 0.18944594630245434\n",
            "epoch_val_loss 0.3040047499040763\n",
            "train_losses_iters 0.11977645009756088\n",
            "val_loss 0.2975579227010409\n",
            "train_losses_iters 0.10160478204488754\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  7%|▋         | 2/30 [00:44<10:26, 22.37s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch_train_loss 0.10966601803646249\n",
            "epoch_val_loss 0.3007813363025586\n",
            "train_losses_iters 0.1000770777463913\n",
            "val_loss 0.2691224440932274\n",
            "train_losses_iters 0.0735883116722107\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 10%|█         | 3/30 [01:05<09:51, 21.90s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch_train_loss 0.08133971179693432\n",
            "epoch_val_loss 0.29022837223278153\n",
            "train_losses_iters 0.07713324576616287\n",
            "val_loss 0.25651523160437745\n",
            "train_losses_iters 0.05813998728990555\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 13%|█▎        | 4/30 [01:28<09:35, 22.13s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch_train_loss 0.06260994083042871\n",
            "epoch_val_loss 0.2818000870756805\n",
            "train_losses_iters 0.046905502676963806\n",
            "val_loss 0.2709282288948695\n",
            "train_losses_iters 0.03942718729376793\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 17%|█▋        | 5/30 [01:50<09:08, 21.92s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch_train_loss 0.0534615700401492\n",
            "epoch_val_loss 0.2796257154395183\n",
            "train_losses_iters 0.039168763905763626\n",
            "val_loss 0.2809102162718773\n",
            "train_losses_iters 0.04853525757789612\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 20%|██        | 6/30 [02:10<08:35, 21.48s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch_train_loss 0.0501859330461692\n",
            "epoch_val_loss 0.2798397989115781\n",
            "train_losses_iters 0.05452767759561539\n",
            "val_loss 0.28153128549456596\n",
            "train_losses_iters 0.05130784958600998\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 23%|██▎       | 7/30 [02:31<08:13, 21.44s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch_train_loss 0.04667344667270022\n",
            "epoch_val_loss 0.28008143985200495\n",
            "train_losses_iters 0.03825276345014572\n",
            "val_loss 0.2783099636435509\n",
            "train_losses_iters 0.04403531178832054\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 27%|██▋       | 8/30 [02:53<07:54, 21.55s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch_train_loss 0.041571769263532204\n",
            "epoch_val_loss 0.27986000532594824\n",
            "train_losses_iters 0.03835386037826538\n",
            "val_loss 0.2738184481859207\n",
            "train_losses_iters 0.027098238468170166\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 30%|███       | 9/30 [03:15<07:30, 21.48s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch_train_loss 0.0396836246284893\n",
            "epoch_val_loss 0.2791887211992785\n",
            "train_losses_iters 0.04622892290353775\n",
            "val_loss 0.270677267263333\n",
            "train_losses_iters 0.03935813531279564\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 33%|███▎      | 10/30 [03:37<07:12, 21.62s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch_train_loss 0.03707270393684759\n",
            "epoch_val_loss 0.27833757580568397\n",
            "train_losses_iters 0.042214568704366684\n",
            "val_loss 0.27326876545945805\n",
            "train_losses_iters 0.028616925701498985\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 37%|███▋      | 11/30 [03:59<06:53, 21.78s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch_train_loss 0.03880852508216591\n",
            "epoch_val_loss 0.2778767748651179\n",
            "train_losses_iters 0.03389162942767143\n",
            "val_loss 0.2737773743768533\n",
            "train_losses_iters 0.02612898498773575\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 40%|████      | 12/30 [04:21<06:33, 21.85s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch_train_loss 0.034469563608704984\n",
            "epoch_val_loss 0.2775351581577626\n",
            "train_losses_iters 0.04248647019267082\n",
            "val_loss 0.26850904151797295\n",
            "train_losses_iters 0.044311508536338806\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 43%|████▎     | 13/30 [04:43<06:14, 22.01s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch_train_loss 0.034539236671338645\n",
            "epoch_val_loss 0.27684084149316335\n",
            "train_losses_iters 0.031111594289541245\n",
            "val_loss 0.2681332317491372\n",
            "train_losses_iters 0.02934352122247219\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 47%|████▋     | 14/30 [05:06<05:56, 22.31s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch_train_loss 0.03300908015314805\n",
            "epoch_val_loss 0.2762188693685901\n",
            "train_losses_iters 0.028144828975200653\n",
            "val_loss 0.26620429878433544\n",
            "train_losses_iters 0.03578031808137894\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 50%|█████     | 15/30 [05:28<05:31, 22.12s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch_train_loss 0.03388302921617435\n",
            "epoch_val_loss 0.27555123132963977\n",
            "train_losses_iters 0.020379364490509033\n",
            "val_loss 0.2658570433656375\n",
            "train_losses_iters 0.026192035526037216\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 53%|█████▎    | 16/30 [05:50<05:09, 22.13s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch_train_loss 0.032908304831234074\n",
            "epoch_val_loss 0.27494534458188963\n",
            "train_losses_iters 0.02278626710176468\n",
            "val_loss 0.26575810834765434\n",
            "train_losses_iters 0.029938986524939537\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 57%|█████▋    | 17/30 [06:12<04:47, 22.13s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch_train_loss 0.03187906483219842\n",
            "epoch_val_loss 0.27440491892105223\n",
            "train_losses_iters 0.03535044193267822\n",
            "val_loss 0.2657393639286359\n",
            "train_losses_iters 0.02773093618452549\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 60%|██████    | 18/30 [06:34<04:25, 22.09s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch_train_loss 0.03340007718336784\n",
            "epoch_val_loss 0.27392349919925135\n",
            "train_losses_iters 0.02535330504179001\n",
            "val_loss 0.2662106566131115\n",
            "train_losses_iters 0.052838344126939774\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 63%|██████▎   | 19/30 [06:56<04:03, 22.13s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch_train_loss 0.030136805311855624\n",
            "epoch_val_loss 0.2735175601157703\n",
            "train_losses_iters 0.024133067578077316\n",
            "val_loss 0.2652721007664998\n",
            "train_losses_iters 0.04846799001097679\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 67%|██████▋   | 20/30 [07:18<03:39, 21.99s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch_train_loss 0.03212273840681981\n",
            "epoch_val_loss 0.27310528714830673\n",
            "train_losses_iters 0.033007554709911346\n",
            "val_loss 0.2656647513310115\n",
            "train_losses_iters 0.03987538442015648\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 70%|███████   | 21/30 [07:40<03:18, 22.10s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch_train_loss 0.033537859917949824\n",
            "epoch_val_loss 0.27275097591891173\n",
            "train_losses_iters 0.03195161372423172\n",
            "val_loss 0.26183613389730453\n",
            "train_losses_iters 0.02812958136200905\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 73%|███████▎  | 22/30 [08:02<02:55, 21.98s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch_train_loss 0.03167764394212577\n",
            "epoch_val_loss 0.27225484673611144\n",
            "train_losses_iters 0.03745385631918907\n",
            "val_loss 0.2603571017583211\n",
            "train_losses_iters 0.03445829078555107\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 77%|███████▋  | 23/30 [08:25<02:36, 22.40s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch_train_loss 0.03364339979144476\n",
            "epoch_val_loss 0.2717375534762075\n",
            "train_losses_iters 0.026023486629128456\n",
            "val_loss 0.26409971465667087\n",
            "train_losses_iters 0.024759670719504356\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 80%|████████  | 24/30 [08:48<02:14, 22.38s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch_train_loss 0.032213530108585194\n",
            "epoch_val_loss 0.2714193101920601\n",
            "train_losses_iters 0.028802480548620224\n",
            "val_loss 0.2644703797996044\n",
            "train_losses_iters 0.04072466865181923\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 83%|████████▎ | 25/30 [09:10<01:51, 22.26s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch_train_loss 0.032311480744915494\n",
            "epoch_val_loss 0.2711413529763619\n",
            "train_losses_iters 0.0403682179749012\n",
            "val_loss 0.2687680125236511\n",
            "train_losses_iters 0.023037396371364594\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 87%|████████▋ | 26/30 [09:32<01:28, 22.15s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch_train_loss 0.03465310929311534\n",
            "epoch_val_loss 0.27105007065125764\n",
            "train_losses_iters 0.03509781137108803\n",
            "val_loss 0.2637348845601082\n",
            "train_losses_iters 0.0306241512298584\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 90%|█████████ | 27/30 [09:54<01:06, 22.14s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch_train_loss 0.03493762177304696\n",
            "epoch_val_loss 0.2707791378330669\n",
            "train_losses_iters 0.027685103937983513\n",
            "val_loss 0.26754743978381157\n",
            "train_losses_iters 0.01863890141248703\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 93%|█████████▎| 28/30 [10:16<00:44, 22.07s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch_train_loss 0.03171652741730213\n",
            "epoch_val_loss 0.2706637200455935\n",
            "train_losses_iters 0.031927015632390976\n",
            "val_loss 0.27151936416824657\n",
            "train_losses_iters 0.0530737042427063\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 97%|█████████▋| 29/30 [10:38<00:22, 22.21s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch_train_loss 0.03558937305489839\n",
            "epoch_val_loss 0.27069322501534016\n",
            "train_losses_iters 0.03313778340816498\n",
            "val_loss 0.2750374215344588\n",
            "train_losses_iters 0.028403155505657196\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 30/30 [11:00<00:00, 22.01s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch_train_loss 0.030513697130195166\n",
            "epoch_val_loss 0.27083803156597747\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "train_loader = tscp_model.train_dataloader()\n",
        "val_loader = tscp_model.val_dataloader()\n",
        "\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "epoch_wise_sim = []\n",
        "epoch_wise_neg = []\n",
        "for epoch in tqdm(range(EPOCHS)):\n",
        "    \n",
        "    iteration = 0\n",
        "    train_losses_iters = []\n",
        "    step_wise_sim = []\n",
        "    step_wise_neg = []\n",
        "    \n",
        "    for index, batch in enumerate(train_loader):\n",
        "            \n",
        "        loss, sim, neg = tscp_model.training_step(batch, index)\n",
        "        train_losses_iters.append(float(loss))\n",
        "        step_wise_sim.append(float(sim))\n",
        "        step_wise_neg.append(float(neg))\n",
        "        if not iteration % 50:\n",
        "            print(\"train_losses_iters\", train_losses_iters[-1])\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        lr.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        iteration += 1\n",
        "\n",
        "        \n",
        "        if not iteration % 50:\n",
        "            tscp_model.eval()\n",
        "            vall = []\n",
        "            with torch.no_grad():\n",
        "                for c, b in enumerate(val_loader):\n",
        "                    val_loss, _, _ = tscp_model.validation_step(b, c)\n",
        "                    vall.append(float(val_loss.detach()))\n",
        "                    if c>10:\n",
        "                        break\n",
        "                print(\"val_loss\", np.mean(vall))\n",
        "                val_losses.append(np.mean(vall))\n",
        "              \n",
        "            \n",
        "            tscp_model.train()\n",
        "    print(\"epoch_train_loss\", np.mean(train_losses_iters))  \n",
        "    train_losses.append(np.mean(train_losses_iters))\n",
        "    print(\"epoch_val_loss\", np.mean(val_losses))\n",
        "    epoch_wise_sim.append(np.mean(step_wise_sim))\n",
        "    epoch_wise_neg.append(np.mean(step_wise_neg))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "0RikDZy3HjyA"
      },
      "outputs": [],
      "source": [
        "torch.save({\n",
        "            'model_state_dict': tscp_model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            }, './tscp'+str(WIN)+str(CODE_SIZE)+str(BATCH_SIZE)+'_opt.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "q3T7FqOhHnDA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "425ee5d9-272c-429c-ae0d-1fe36e969663"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average similarity for test set : Reps : 0.9715154767036438\n",
            "tn 442, fp 0, fn 22, tp 4 ----- f1-score 0.2666666666666667\n",
            "SEQ : Pos 7, fp 0, fn 5, tp 2 ----- f1-score 0.4444444444444444\n"
          ]
        }
      ],
      "source": [
        "x_test, lbl_test = test_ds[:,1:], test_ds[:,0]\n",
        "\n",
        "num = x_test.shape[0]\n",
        "lbl_test = np.array(lbl_test).reshape((lbl_test.shape[0], 1))\n",
        "history = prep_model(torch.from_numpy(x_test[:, 0:WIN].reshape((num, 1, WIN))).float())\n",
        "future = prep_model(torch.from_numpy(x_test[:, WIN:].reshape((num, 1, WIN))).float())\n",
        "pred_out = np.concatenate((lbl_test, history.detach().numpy(), future.detach().numpy()), 1)\n",
        "rep_sim = _cosine_simililarity_dim1(history, future)\n",
        "\n",
        "print('Average similarity for test set : Reps : {}'.format(np.mean(rep_sim.detach().numpy())))\n",
        "gt = np.zeros(lbl_test.shape[0])\n",
        "gt[np.where((lbl_test > int(2 * WIN * 0.15)) & (lbl_test < int(2 * WIN * 0.85)))[0]] = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sQUBGXjWMJVL",
        "outputId": "6e3cf348-63ab-4686-d860-4028a272c8d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tn 436, fp 6, fn 8, tp 18 ----- f1-score 0.7199999999999999\n",
            "SEQ : Pos 7, fp 4, fn 1, tp 6 ----- f1-score 0.7058823529411765\n"
          ]
        }
      ],
      "source": [
        "result = estimate_CPs(rep_sim.detach().numpy(), gt, os.path.join(OUTPUT_PATH, train_name),\n",
        "                    os.path.join(OUTPUT_PATH, \"Evaluation.txt\"),\n",
        "                    metric='cosine', threshold=epoch_wise_sim[-1] - epoch_wise_neg[-1])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}